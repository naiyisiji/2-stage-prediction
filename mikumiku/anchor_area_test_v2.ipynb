{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== tesnor lib ============================================ # \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# ====================================================================================== # \n",
    "\n",
    "from torch_cluster import radius_graph\n",
    "from torch_cluster import radius\n",
    "from torch_geometric.data import Batch\n",
    "import matplotlib.pyplot as plt\n",
    "from layers import FourierEmbedding\n",
    "from utils.circle import minimum_enclosing_circle\n",
    "from modules.encoder import QCNetEncoder\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from layers.attention_layer import AttentionLayer\n",
    "from utils.geometry import angle_between_2d_vectors\n",
    "\n",
    "# ============================== building transformer lib ============================== #  \n",
    "from transformers import BertModel, BertConfig\n",
    "# ====================================================================================== # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTR(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, num_heads):\n",
    "        super(BertTR, self).__init__()\n",
    "        \n",
    "        config = BertConfig(\n",
    "            hidden_size=hidden_dim,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=hidden_dim * 4,\n",
    "            max_position_embeddings=512,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "        )\n",
    "        self.encoder = BertModel(config)\n",
    "\n",
    "    def forward(self, x, valid_mask):\n",
    "        # x: [B, t, dim]\n",
    "        # valid_mask: [B, t]\n",
    "        \n",
    "        # Generate time mask\n",
    "        seq_length = x.size(1)\n",
    "        time_mask = torch.triu(torch.ones((seq_length, seq_length)), diagonal=1).bool().to(x.device)\n",
    "        \n",
    "        # Combine time mask with valid_mask\n",
    "        extended_valid_mask = valid_mask[:, None, None, :].to(x.device)\n",
    "        combined_mask = time_mask[None, None, :, :] | (~extended_valid_mask)\n",
    "        \n",
    "        # Attention mask for BERT expects float values (0 or -inf)\n",
    "        attention_mask = combined_mask.float() * -1e9\n",
    "        # Pass through the transformer encoder\n",
    "        outputs = self.encoder(inputs_embeds=x, attention_mask=attention_mask)\n",
    "        \n",
    "        return outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class Agent_Self_Attn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_head = 1) -> None:\n",
    "        super(Agent_Self_Attn, self).__init__()\n",
    "    \n",
    "    def forward(self, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent 2 query cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_2_query_attn(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Agent_2_query_attn, self).__init__()\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Area_Prior(nn.Module):\n",
    "    def __init__(self,\n",
    "                 agent_self_attn_hidden_dim = 128,\n",
    "                 agent_self_attn_layer_num = 2,\n",
    "                 agent_self_attn_num_head = 8,\n",
    "                 agent_self_attn_embed_num_freq_bands = 64,\n",
    "                 agent_self_attn_embed_hidden_dim = 128,\n",
    "                 history_step = 50,\n",
    "                 prediction_step = 60,\n",
    "                 input_dim = 2,\n",
    "                 agent_embed_input_dim = 4) -> None:\n",
    "        super(Area_Prior, self).__init__()\n",
    "        self.history_step = history_step\n",
    "        self.input_dim = input_dim\n",
    "        self.agent_self_attn_hidden_dim = agent_self_attn_hidden_dim\n",
    "        self.agent_self_attn = Agent_self_attn(agent_self_attn_hidden_dim,\n",
    "                                               agent_self_attn_layer_num,\n",
    "                                               agent_self_attn_num_head)\n",
    "        # self.agent2query_attn = Agent_2_query_attn()\n",
    "        self.linear = nn.Linear(1,1).to(device=torch.device('cuda'))\n",
    "        self.type_a_emb = nn.Embedding(10, agent_self_attn_hidden_dim)\n",
    "        self.x_a_emb = FourierEmbedding(input_dim=agent_embed_input_dim, \n",
    "                                        hidden_dim=agent_self_attn_embed_hidden_dim, \n",
    "                                        num_freq_bands=agent_self_attn_embed_num_freq_bands)\n",
    "        \n",
    "        self.proj_geo = nn.Linear(agent_self_attn_hidden_dim, input_dim)\n",
    "        self.proj_t = nn.Linear(history_step, prediction_step)\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        # ============================== setting data ============================== # \n",
    "        pred_mask = data['agent']['predict_mask'].any(dim=-1, keepdim=True).squeeze()\n",
    "        valid_mask_history = data['agent']['valid_mask'][pred_mask,:self.history_step]\n",
    "        #agent_data_pos = data['agent']['position'][pred_mask,:self.history_step,:self.input_dim]\n",
    "        #agent_data_heading = data['agent']['heading'][pred_mask,:self.history_step,:self.input_dim]\n",
    "\n",
    "        pos_a = data['agent']['position'][:, :self.history_step, :self.input_dim].contiguous()\n",
    "        motion_vector_a = torch.cat([pos_a.new_zeros(data['agent']['num_nodes'], 1, self.input_dim),\n",
    "                                     pos_a[:, 1:] - pos_a[:, :-1]], dim=1)\n",
    "        head_a = data['agent']['heading'][:, :self.history_step].contiguous()\n",
    "        head_vector_a = torch.stack([head_a.cos(), head_a.sin()], dim=-1)\n",
    "        vel = data['agent']['velocity'][:, :self.history_step, :self.input_dim].contiguous()\n",
    "        categorical_embs = [self.type_a_emb(data['agent']['type'].long()).repeat_interleave(repeats=self.history_step,dim=0)]\n",
    "        agent_geo_feat = torch.stack(\n",
    "                [torch.norm(motion_vector_a[:, :, :2], p=2, dim=-1),\n",
    "                 angle_between_2d_vectors(ctr_vector=head_vector_a, nbr_vector=motion_vector_a[:, :, :2]),\n",
    "                 torch.norm(vel[:, :, :2], p=2, dim=-1),\n",
    "                 angle_between_2d_vectors(ctr_vector=head_vector_a, nbr_vector=vel[:, :, :2])], dim=-1)\n",
    "        agent_feat = self.x_a_emb(continuous_inputs=agent_geo_feat.view(-1, agent_geo_feat.size(-1)), categorical_embs=categorical_embs)\n",
    "        agent_feat = agent_feat.view(-1, self.history_step, self.agent_self_attn_hidden_dim)\n",
    "        # ============================================================================= # \n",
    "\n",
    "        # ============================== extract self_attn feat ============================== #\n",
    "        agent_feat = self.agent_self_attn(agent_feat, valid_mask_history)\n",
    "        # ==================================================================================== # \n",
    "\n",
    "        agent_feat_t = self.proj_t(agent_feat.permute(0,2,1)).permute(0,2,1)\n",
    "        agent_feat_pos = self.proj_geo(agent_feat_t)\n",
    "        return agent_feat_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, lambda_area=1.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.lambda_area = lambda_area\n",
    "\n",
    "    def forward(self, points_A, points_B):\n",
    "        distribution_loss = self.compute_distribution_loss(points_A, points_B)\n",
    "        area_loss = self.compute_area_loss(points_A)\n",
    "        total_loss = distribution_loss + self.lambda_area * area_loss * 0.1\n",
    "        return total_loss\n",
    "\n",
    "    def compute_distribution_loss(self, points_A, points_B):\n",
    "        dist_matrix = torch.cdist(points_A, points_B)\n",
    "        min_dist, _ = dist_matrix.min(dim=1)\n",
    "        return min_dist.mean()\n",
    "\n",
    "    def compute_area_loss(self, points_A):\n",
    "        points_A = points_A.detach().cpu().numpy()\n",
    "        if len(points_A) < 3:  # ConvexHull requires at least 3 points\n",
    "            return torch.tensor(0.0)\n",
    "        hull = ConvexHull(points_A)\n",
    "        return torch.tensor(hull.volume, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\pytorch_py3_9\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 50, 50])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Wrong shape for input_ids (shape torch.Size([30, 50])) or attention_mask (shape torch.Size([22, 1, 50, 50]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, loader_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(loader,\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(loader))):\n\u001b[0;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 22\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43marea_prior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     pred_mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     24\u001b[0m     gt \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m][pred_mask,:,:]\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\pytorch_py3_9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36mArea_Prior.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     50\u001b[0m agent_feat \u001b[38;5;241m=\u001b[39m agent_feat\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_self_attn_hidden_dim)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# ============================================================================= # \u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# ============================== extract self_attn feat ============================== #\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m agent_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_self_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_mask_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# ==================================================================================== # \u001b[39;00m\n\u001b[0;32m     57\u001b[0m agent_feat_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_t(agent_feat\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\pytorch_py3_9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36mAgent_self_attn.forward\u001b[1;34m(self, x, valid_mask)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(attention_mask[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Pass through the transformer encoder\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\pytorch_py3_9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\pytorch_py3_9\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1109\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1103\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m   1104\u001b[0m             attention_mask, embedding_output\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39mseq_length\n\u001b[0;32m   1105\u001b[0m         )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m-> 1109\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_extended_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\pytorch_py3_9\\lib\\site-packages\\transformers\\modeling_utils.py:1070\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[1;34m(self, attention_mask, input_shape, device, dtype)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape for input_ids (shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) or attention_mask (shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;66;03m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;66;03m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;66;03m# positions we want to attend and the dtype's smallest value for masked positions.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;66;03m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;66;03m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m extended_attention_mask\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdtype)  \u001b[38;5;66;03m# fp16 compatibility\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong shape for input_ids (shape torch.Size([30, 50])) or attention_mask (shape torch.Size([22, 1, 50, 50]))"
     ]
    }
   ],
   "source": [
    "from dataset_prepare.argoverse_v2_dataset import ArgoverseV2Dataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "dataset = ArgoverseV2Dataset('D:\\\\argoverse2', 'train', None, None, None)\n",
    "loader = DataLoader(dataset,batch_size=1,shuffle=False)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "area_prior = Area_Prior(128,1,8).to(device)\n",
    "optimizer = torch.optim.SGD(area_prior.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = CustomLoss(lambda_area=0.1)\n",
    "epoch = 1\n",
    "\n",
    "min_loss = np.inf\n",
    "best_model_path = 'C:\\\\Users\\\\Lenovo\\\\OneDrive - City University of Hong Kong - Student\\\\Desktop\\\\mikumiku\\\\mikumiku\\\\best_model.pth'\n",
    "\n",
    "\n",
    "for epoch_i in range(epoch):\n",
    "    print(epoch_i)\n",
    "    for data, loader_num in zip(loader,range(len(loader))):\n",
    "        loss = 0\n",
    "        pred = area_prior(data = data.to(device))\n",
    "        pred_mask = data['agent']['predict_mask'].any(dim=-1, keepdim=True).squeeze()\n",
    "        gt = data['agent']['position'][pred_mask,:,:]\n",
    "        for each_pred, each_gt, data_in_loader_i in zip(pred,gt,range(gt.shape[0])):\n",
    "            each_gt = each_gt[50:,:][data['agent']['predict_mask'][pred_mask][data_in_loader_i,50:],:2]\n",
    "            loss += criterion(each_pred,each_gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ====================================save best model param==================================== #\n",
    "        if loss.item() < min_loss:\n",
    "            min_loss = loss.item()\n",
    "            torch.save(area_prior.state_dict(), best_model_path)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Area_Anchor(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Area_Anchor, self).__init__()\n",
    "\n",
    "        # 1. agent self_attn 提取feature\n",
    "        # 2. 设计anchor-free的 detr模型推理可行域点集，是预训练的\n",
    "    def forward(self, data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pts_Anchor(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Pts_Anchor, self).__init__()\n",
    "        # 3. 初始化 Pts-query-content + Pts-query-pos, 并自注意力结合特征， query-pos是几何上的\n",
    "        # 4. Pts-Query 对 最后一个历史步做解码得到新的 pos_ 和 content_， 并以 pos_new = pos+pos_ ;content = content_ + content 迭代\n",
    "        # 5. 组合迭代结果对area_archor做ca 实现 时序关联\n",
    "        # 6. refine\n",
    "    \n",
    "    def forward(self, data):\n",
    "        pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
